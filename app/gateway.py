from flask import (Flask,request,Response,abort,jsonify,g)  
import json
from datetime import datetime
from cachetools import TTLCache
import logging
import requests

app = Flask(__name__)

# Configuro rutas de microservicios
AUTH_SERVICE_URL = 'http://localhost:5001/authenticate'
LOG_SERVICE_URL = 'http://localhost:5003/log'
PREDICT_SERVICE_URL = 'http://localhost:5002/predict'

# Configura el nivel de registro (DEBUG, INFO, etc.)
logging.basicConfig(level=logging.DEBUG)


CACHE_SIZE = 100
CACHE_EXPIRATION_TIME = 60  # segundos
cache = TTLCache(maxsize=CACHE_SIZE, ttl=CACHE_EXPIRATION_TIME)

@app.route('/predict', methods=['GET'])
def predict():
    try: 
        start_time = datetime.now().timestamp()

        # Procedo a autenticar con el microservicio
        api_key = request.headers.get('Authorization')
        authentication_result = requests.post(AUTH_SERVICE_URL, headers={'Authorization': api_key}, json={"start_time": start_time})
        if authentication_result.status_code != 200:
            return jsonify({"respuesta": "Error de autenticación"}), 403
        # Autenticación correcta
        else:
            params = {
                "colesterol": float(request.args.get("colesterol", 0)),
                "presion": float(request.args.get("presion", 0)),
                "glucosa": float(request.args.get("glucosa", 0)),
                "edad": float(request.args.get("edad", 0)),
                "sobrepeso": float(request.args.get("sobrepeso", 0)),
                "tabaquismo": float(request.args.get("tabaquismo", 0))
             }
        
            cache_key = json.dumps(params)
            
            if cache_key in cache:
                result = cache[cache_key]
                #print("Usando la caché disponible:", cache_key)
                logging.debug("Usando la caché disponible: %s", cache_key)
            else:
                json_data = params
                result = requests.post(PREDICT_SERVICE_URL, json=json_data)
                result = result.json()
                
                cache[cache_key] = result

                log_json = {
                    "params": json_data,
                    "response": result,
                    "start_time": start_time,
                    "user_info": authentication_result.json()["user_info"]
                }
                log_result = requests.post(LOG_SERVICE_URL, json=log_json)
            
                #print("Nueva caché disponible:", cache_key)
                logging.debug("Nueva caché disponible: %s", cache_key)

        return result

    except ValueError as e:
        return {"Error en la función predict al colocar un valor incorrecto": str(e)}, 400
    except Exception as e:
        return {"Error en la función predict": str(e)}, 500

@app.route('/train_model', methods=['GET'])
def train():
    try:
        """tp_final_topicos2.ipynb

        Automatically generated by Colaboratory.

        Original file is located at
            https://colab.research.google.com/drive/1wzFBkpgvHLcpnaoYZS7NLwO11kPrR_Pb
        """

        """## Paso 1: Cargar los datos.
        Levantamos los datos de los pacientes
        """

        # Read data from file

        import numpy as np
        import pandas as pd
        import joblib

        file_name = './../data/datos_de_pacientes_5000.csv'
        data = pd.read_csv(file_name, index_col=0)

        print("Datos de pacientes")
        print("-----------------------")
        print(data)

        """## Paso 2: Preprocesar los datos.
        Separamos los datos de entrada de las etiquetas
        Separamos conjuntos de training, validación y testing según sea necesario
        """

        # Date preprocessing
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import MinMaxScaler

        # Scaling numerical variables
        scaler = MinMaxScaler()

        # Separate the data from the target labels
        X = data.drop(['riesgo_cardiaco'], axis=1)
        y = np.array(data['riesgo_cardiaco'])

        # Split the data into training and testing sets
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

        # For training set
        scaled_X_train = scaler.fit_transform(X_train)
        scaled_X_train = pd.DataFrame(scaled_X_train, columns=X_train.columns)

        # For testing set
        scaled_X_test = scaler.transform(X_test)
        scaled_X_test = pd.DataFrame(scaled_X_test, columns=X_test.columns)

        print("Set de training")
        print("---------------")
        print(scaled_X_train)

        print("Set de test")
        print("------------")
        print(scaled_X_test)

        """##Paso 3: Armo la red"""

        # Build the Neural Network
        import numpy as np
        from tensorflow import keras
        from keras.models import Sequential
        from keras.layers import Dense
        from tensorflow.keras.optimizers import Adam

        # Create the model
        model = Sequential()

        # 6 INPUT (colesterol, presión, glucosa, edad, sobrepeso, tabaquismo)
        model.add(Dense(50, input_shape=(6,), activation='relu', kernel_initializer='uniform'))
        model.add(Dense(25, activation='relu', kernel_initializer='random_normal'))
        model.add(Dense(35, activation='relu', kernel_initializer='random_normal'))
        model.add(Dense(1, activation='sigmoid')) # Sigmoid activation in the output layer

        # Compile
        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01))
        model.summary()

        """##Paso 4: Entreno la red neuronal"""

        # Training
        historicalModel = model.fit(scaled_X_train, y_train, validation_split=0.2, epochs=200, batch_size=32, verbose=2)

        """##Paso 5: Evaluo la red"""

        # Make predictions with the model
        y_pred = model.predict(scaled_X_test)

        import matplotlib.pyplot as plt

        # Visualización de la pérdida
        plt.plot(historicalModel.history['loss'], label='Training Loss')
        plt.plot(historicalModel.history['val_loss'], label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.show()

        # Evaluate
        result = model.evaluate(scaled_X_test, y_test)
        print("Evaluate the model: ",result)

        plt.xlabel("# Epoca")
        plt.ylabel("Magnitud de pérdida")
        plt.plot(historicalModel.history["loss"])
        plt.legend()
        plt.show()

        print("Datos a predecir:")
        print(X_train[:3])
        print("-----------------")

        print("Resultados obtenidos:")
        print(y_pred[:3])
        print("Valores correctos:")
        print(y_test[:3])

        model.save("model.keras")
        model = keras.models.load_model("model.keras")

        # Save the scaler
        joblib.dump(scaler, "scaler.joblib")

        return jsonify({"respuesta": "Entrenamiento de modelo finalizado correctamente"}),200
    
    except Exception as e:
        return jsonify({"Error entrenando el modelo": str(e)}), 500
    
if __name__ == '__main__':
    app.run(port=5000, debug=True)
